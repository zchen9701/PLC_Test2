import sys
from LexicalAnalyzer import *

next_token = None
l = None


def expr():
  global next_token
  global l
  print("Enter <expr>")
  term()
  while next_token.get_token().value == DefineToken.ADDITION.value or \
          next_token.get_token().value == DefineToken.MINUS.value:
    next_token = l.lex()
    term()
  print("Exit <expr>")


# factor
# Parses strings in the language generated by the rules:
# <factor> -> ID
# <factor> -> INT_CONSTANT
# <factor> -> ( <expr> )
def factor():
  global next_token
  global l
  print("Enter <factor>")
  if next_token.get_token().value == DefineToken.IDENT.value or \
     next_token.get_token().value == DefineToken.WHOLE_NUM.value:
    next_token = l.lex()
  else:  # if the RHS is ( <expr> ), pass over (, call expr, check for )
    if next_token.get_token().value == DefineToken.LEFTPAREN.value:
      next_token = l.lex()
      expr()
      if next_token.get_token().value == DefineToken.RIGHTPAREN.value:
        next_token = l.lex()
      else:
        error("Expecting RIGHTPAREN")
        sys.exit(-1)
    else:
      error("Expecting LEFTPAREN")
      sys.exit(-1)
  print("Exit <factor>")


def error(s):
  print("SYNTAX ERROR: " + s)

# term
# Parses strings in the language generated by the rule:
# <term> : <factor> {(*|/) <factor>}


def term():
  global next_token
  global l
  print("Enter <term>")
  factor()
  while next_token.get_token().value == DefineToken.TIMES.value or \
          next_token.get_token().value == DefineToken.DIVISION.value:
    next_token = l.lex()
    factor()
  print("Exit <term>")


# expr
# Parses strings in the language generated by the rule:
# <expr> : <term> {(+|-) <term>}


def main():
  global next_token
  global l
  l = Lexer(sys.argv[1])
  next_token = l.lex()

  expr()
  if next_token.get_token().value == DefineToken.EOF.value:
    print("AWESOME! PARSE SUCCESSFULLY!!!")
  else:
    print("OH NO... SEEMS LIKE PARSE FAILED...")


main() 